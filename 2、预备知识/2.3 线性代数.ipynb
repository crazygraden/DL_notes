{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "sum_a = A.sum(axis=1, keepdims=True)\n",
    "A,A.cumsum(axis=1)\n",
    "# cumsum计算沿某个轴累积总和，比如axis=0\n",
    "# 计算得出的结果(2,2)表示计算的(0,2)、(1,2)、(2,2)所得\n",
    "\n",
    "# 点积 torch.dot只能对一维数据进行处理\n",
    "x = torch.arange(3,dtype=torch.float32)\n",
    "y = torch.arange(3,dtype=torch.float32)\n",
    "x,y,torch.dot(x,y)\n",
    "# 矩阵向量积 torch.mv, axis=1 维数必须一致\n",
    "X = torch.arange(6,dtype=torch.float32).reshape(2,3)\n",
    "y = torch.arange(3,dtype=torch.float32)\n",
    "X,y,torch.mv(X,y)\n",
    "# 矩阵-矩阵乘法 (attention!!!不要和哈达玛积混淆，一个是行列对应相乘后相加，一个是一个个元素相乘)\n",
    "X = torch.arange(6,dtype=torch.float32).reshape(2,3)\n",
    "Y = torch.arange(6,dtype=torch.float32).reshape(3,2)\n",
    "X,Y,torch.mm(X,Y)\n",
    "# L_2范数，x_i的平方相加再开根号所得\n",
    "# L_2范数常省略下标2，也就是说||x||等价于||x||_2\n",
    "a = torch.tensor([3.0, -4.0])\n",
    "torch.norm(a)  # tensor(5.)\n",
    "# L_1范数，x_i的绝对值再相加\n",
    "torch.abs(a).sum() # tensor(7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]), tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]), tensor(4.8990), tensor(3.4641))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "# 1、证明矩阵的转置的转置是矩阵本身\n",
    "X = torch.arange(16,dtype=torch.float32).reshape(4,4)\n",
    "X_T = X.T\n",
    "orig_X = X_T.T\n",
    "X == orig_X\n",
    "\n",
    "# 2、给定两个矩阵A和B，证明它们“转置的和”==“和的转置”\n",
    "A = torch.arange(4,dtype=torch.float32).reshape(2,2)\n",
    "B = torch.arange(4,dtype=torch.float32).reshape(2,2)\n",
    "C = A * B\n",
    "A.T * B.T == C.T\n",
    "\n",
    "# 3、给定任意方阵A，A和A.T总是对称的嘛？为什么？\n",
    "'''\n",
    "(A + A.T)^T = A.T + A.T.T = A.T+A = A+A.T\n",
    "'''\n",
    "\n",
    "# 4、本节中定义了形状(2,3,4)的张量X。len(X)的输出结果是什么？\n",
    "X = torch.arange(24,dtype=torch.float32).reshape(2,3,4)\n",
    "# print(len(X))  # 2\n",
    "\n",
    "# 5、对于任意形状的张量X,len(X)是否总是对应于X特定轴的长度?这个轴是什么?\n",
    "# 总是等于axis=0的长度 \n",
    "\n",
    "# 6、运行A/A.sum(axis=1)，看看会发生什么。请分析一下原因？\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A,A.sum(axis=1)\n",
    "# A/A.sum(axis=1) 广播机制只能在两者维度相同的情况下\n",
    "# RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n",
    "\n",
    "# 7、考虑一个具有形状(2,3,4)的张量，在轴0、1、2上的求和输出是什么形状?\n",
    "X = torch.arange(24,dtype=torch.float32).reshape(2,3,4)\n",
    "# print(X)\n",
    "# (torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))\n",
    "X.sum(axis=0),X.sum(axis=1),X.sum(axis=2)\n",
    "\n",
    "# 8、为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?\n",
    "A, B = torch.ones(2,3,4), torch.ones(3, 4)\n",
    "outputs1 = torch.linalg.norm(A)  # 2x3x4张量\n",
    "outputs2 = torch.linalg.norm(B)  # 3x4张量\n",
    "A, B, outputs1, outputs2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_study",
   "language": "python",
   "name": "my_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
