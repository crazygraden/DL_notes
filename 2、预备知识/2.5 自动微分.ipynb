{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "x.grad\n",
    "y = 2*torch.dot(x,x)\n",
    "y.backward()\n",
    "x.grad\n",
    "x.grad == 4 * x\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x*x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 1., 4., 9.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分离计算\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(y)\n",
    "u = y.detach()\n",
    "print(u)   # u和y一样，只是丢弃了计算图中如何计算y的信息\n",
    "# tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
    "# tensor([0., 1., 4., 9.])\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1853, requires_grad=True)\n",
      "tensor(1118.8765, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python控制流的梯度计算\n",
    "\n",
    "def f(a):\n",
    "    b = a*2\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    return c\n",
    "\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "print(a)\n",
    "d = f(a)\n",
    "print(d)\n",
    "d.backward()\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([0.0000, 0.2673, 0.5345, 0.8018])\n",
      "tensor([0.0000, 0.5345, 1.0690, 1.6036])\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([512., 512., 512., 512.])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 1、为什么计算二阶导数比一阶导数的开销要更大\n",
    "'''\n",
    "· 计算图的构建：在计算一阶导数时，PyTorch 只需要构建一次计算图，然后根据需要进行反向传播即可。而在计算二阶导数时，需要构建两次计算图，第一次是计算一阶导数，第二次是计算一阶导数的导数，也就是二阶导数。因此，计算二阶导数需要更多的计算图构建操作。\n",
    "· 内存占用：计算一阶导数时，PyTorch 只需要保留一阶导数的梯度值，而计算二阶导数时需要保留一阶导数和二阶导数的梯度值，这会占用更多的内存。\n",
    "· 计算量增加：计算二阶导数需要对一阶导数进行额外的计算，这增加了计算量和计算时间。\n",
    "'''\n",
    "\n",
    "\n",
    "# 2、在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "x = torch.arange(4.0,requires_grad=True)\n",
    "y = torch.dot(x,x).sqrt()\n",
    "print(x)\n",
    "y.backward(retain_graph=True) \n",
    "# <- y.backward() \n",
    "'''\n",
    "在运行反向传播函数之后，\n",
    "立即再次运行它，会报错，\n",
    "提示“尝试第二次反向传播\n",
    "（或者在释放保存的张量后直接访问它们）。\n",
    "调用.backward()或autograd.grad()时，\n",
    "将释放图中保存的中间值。\n",
    "如果需要第二次执行反向传播，\n",
    "或者在反向传播调用后需要访问保存的张量，\n",
    "请指定retain_graph=True。\n",
    "'''\n",
    "print(x.grad)\n",
    "try:\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# 3、在控制流的例子中，我们计算d关于a的导数，如果将变量a更改为随机向量或矩阵，会发生什么？\n",
    "def f(a):\n",
    "    b = a*2\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    return c\n",
    "\n",
    "\n",
    "a = torch.arange(4.0, requires_grad=True)\n",
    "# 出现错误：grad can be implicitly created only for scalar outputs\n",
    "# PyTorch只能对标量结果求梯度\n",
    "print(a)\n",
    "d = f(a)\n",
    "try:\n",
    "    d.backward(torch.ones_like(d))  # <-d.backward()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5448, grad_fn=<MulBackward0>)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "tensor(1814.9301, grad_fn=<MulBackward0>)\n",
      "tensor(1814.9301, grad_fn=<SumBackward0>)\n",
      "tensor(1024.)\n"
     ]
    }
   ],
   "source": [
    "# 4、重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "def f(a):\n",
    "    b = a*2\n",
    "    print(b)\n",
    "    while b.norm()<1000:\n",
    "        print('1')\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    print(c)\n",
    "    return c.sum()\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "print(d)\n",
    "d.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 5、f(x) = six(x), 绘制f(x)和对x求导，其中后者不使用f的导数为cos(x)\n",
    "def f(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    x.requires_grad_()\n",
    "    y = f(x)\n",
    "    # print(\"y:\",y)\n",
    "    # print(\"全1：\", torch.ones_like(y))\n",
    "    y.backward(torch.ones_like(y))\n",
    "    return x.grad\n",
    "\n",
    "x = torch.linspace(-2*torch.pi,2*torch.pi,100)\n",
    "y1 = f(x)\n",
    "y2 = df(x)\n",
    "\n",
    "plt.plot(x.detach().numpy(),y1.detach().numpy(),label='f(x)')\n",
    "plt.plot(x.detach().numpy(),y2.detach().numpy(),lable='df/dx')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studytest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
